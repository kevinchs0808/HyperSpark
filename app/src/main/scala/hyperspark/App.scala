/*
 * This Scala source file was generated by the Gradle 'init' task.
 */
package hyperspark

import org.apache.spark.ml.Model
import org.apache.spark.sql._
import org.apache.spark.{SparkContext, SparkConf}
import org.apache.spark.rdd.RDD
import org.apache.spark.serializer.JavaSerializer
import org.apache.spark.ml.{Pipeline, PipelineModel}
import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}
import org.apache.spark.ml.evaluation.{MulticlassClassificationEvaluator, BinaryClassificationEvaluator}
import org.apache.spark.ml.feature.{IndexToString, StringIndexer, StringIndexerModel, VectorIndexer, VectorIndexerModel, VectorAssembler}
import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}

object App {
  def main(args: Array[String]): Unit = {

    // Create a new Spark session, using 7 cores, with a hypothesis that 1 core is for spark master, and 6 cores are for the spark workers
    val spark = SparkSession
      .builder
      .master("local[7]")
      .config("spark.serializer", "org.apache.spark.serializer.JavaSerializer")
      .appName("DiabetesPrediction")
      .getOrCreate()

    // Load and parse the data file, converting it to a DataFrame.
    // Credit: https://www.kaggle.com/datasets/prosperchuks/health-dataset
    val data = spark.read.option("header", true).option("inferSchema", true).csv("file:///Users/kevinchs0808/Desktop/Data_Science_Projects/HyperSpark/app/data/diabetes_data.csv")
    data.printSchema()


    // Define the target variable
    val yvar = "Diabetes"

    //Since Spark ML algorithm expect a 'label' column, which is in our case 'Diabetes". Let's rename it to 'label'
    val renamed_data = data.withColumnRenamed(yvar, "label")

    //Select columns for preparing training data using VectorAssembler()
    val selectedCols = renamed_data.columns.toArray
    val filteredSelectedCols = selectedCols.filter(! _.contains(yvar))

    // Define the Vector Assembler to combine all values from columns (excluding the target variable) into their numerical vector representation
    // Represent the vectors with a column names "features"
    val vectorAssembler = new VectorAssembler()
          .setInputCols(filteredSelectedCols)
          .setOutputCol("features")

    val vectorized_data = vectorAssembler.transform(renamed_data)
                    .select("label", "features")

    // Split the data based on the target variable's categories before train-test splitting to ensure that each train and test dataset are evenly distributed
    val positive_data: DataFrame = vectorized_data.filter(vectorized_data("label") === 1)
    val negative_data: DataFrame = vectorized_data.filter(vectorized_data("label") === 0)


    // Split the data into training and test sets (30% held out for testing).
    val Array(positive_trainingData: DataFrame, positive_testData: DataFrame) = positive_data.randomSplit(Array(0.7, 0.3))
    val Array(negative_trainingData: DataFrame, negative_testData: DataFrame) = negative_data.randomSplit(Array(0.7, 0.3))


    // Merge back the dataframes such that we get the training and testing data
    val trainingData = positive_trainingData.union(negative_trainingData)
    val testData = positive_testData.union(negative_testData)
    
    // initialize the Random Forest Model
    val rf = new RandomForestClassifier()
              .setLabelCol("label")
              .setFeaturesCol("features")

    // Define a simple pipeline instance
    val pipeline = new Pipeline()
      .setStages(Array(rf))

    // We use a ParamGridBuilder to construct a grid of hyperparameters to search over.
    // With 6 values for rf.maxDepth and 4 values for rf.numTrees,
    // this grid will have 6 x 4 = 24 hyperparameter settings for CrossValidator to choose from.
    val paramGrid = new ParamGridBuilder()
      .addGrid(rf.maxDepth, Array(3, 6, 9, 12, 15, 18))
      .addGrid(rf.numTrees, Array(5, 10, 15, 20))
      .build()

    // We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.
    // This will allow us to jointly choose parameters for all Pipeline stages.
    // A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.
    val cv = new CrossValidator()
      .setEstimator(pipeline)
      .setEvaluator(new MulticlassClassificationEvaluator)
      .setEstimatorParamMaps(paramGrid)
      .setNumFolds(3)  // Use 3+ in practice
      .setParallelism(4) // Evaluate up to 4 sets of hyperparameter in parallel
      .setCollectSubModels(true)

    // Run cross-validation, and choose the best set of hyperparameters.
    val cvModel = cv.fit(trainingData)

    // Make predictions on the Diabetes' Patient Records. cvModel uses the best model found (rfModel).
    val predictions = cvModel.transform(testData)

    // Select example rows to display.
    predictions.select("prediction", "label", "features").show(5)

    // Initialize the performance evaluator instance
    val evaluator = new MulticlassClassificationEvaluator()
      .setPredictionCol("prediction") 
      .setLabelCol("label")
      .setMetricName("precisionByLabel")

    // Calculate the precision
    val precision = evaluator.evaluate(predictions)
    println(s"Precision = ${precision}")

    // Obtain the best model
    val bestmodel = cvModel.bestModel.asInstanceOf[PipelineModel]

    val rfModel = bestmodel.stages.last.asInstanceOf[RandomForestClassificationModel]

    // Print the description of the best model
    println(rfModel.toDebugString)                 
    

    spark.stop()
    
  }

}